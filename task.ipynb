{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2878b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"flask_app\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f426a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f376d9e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o217.load.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYSPARK_SUBMIT_ARGS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--driver-class-path C:/Users/krayn/Documents/Innowise Intership/Innowise_task_4/postgresql-42.4.0.jar --jars C:/Users/krayn/Documents/Innowise Intership/Innowise_task_4/postgresql-42.4.0.jar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython Spark SQL basic example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://localhost:5432/test_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpublic.action\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\readwriter.py:184\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o217.load.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-class-path C:/Users/krayn/Documents/Innowise Intership/Innowise_task_4/postgresql-42.4.0.jar --jars C:/Users/krayn/Documents/Innowise Intership/Innowise_task_4/postgresql-42.4.0.jar'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/test_db\") \\\n",
    "    .option(\"dbtable\", \"public.action\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"root\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ce2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine(\"postgresql+psycopg2://root:root@localhost/test_db?client_encoding=utf8\")\n",
    "pd_actor = pd.read_sql('select * from public.actor', engine)\n",
    "pd_address = pd.read_sql('select * from public.address', engine)\n",
    "pd_category = pd.read_sql('select * from public.category', engine)\n",
    "pd_city = pd.read_sql('select * from public.city', engine)\n",
    "pd_country = pd.read_sql('select * from public.country', engine)\n",
    "pd_custormer = pd.read_sql('select * from public.customer', engine)\n",
    "pd_film = pd.read_sql('select * from public.film', engine)\n",
    "pd_film_actor = pd.read_sql('select * from public.film_actor', engine)\n",
    "pd_film_category = pd.read_sql('select * from public.film_category', engine)\n",
    "pd_invenotory = pd.read_sql('select * from public.inventory', engine)\n",
    "pd_language = pd.read_sql('select * from public.language', engine)\n",
    "pd_payment = pd.read_sql('select * from public.payment', engine)\n",
    "pd_rental = pd.read_sql('select * from public.rental', engine)\n",
    "pd_staff = pd.read_sql('select * from public.staff', engine)\n",
    "pd_store = pd.read_sql('select * from public.store', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3e75d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>staff_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>address_id</th>\n",
       "      <th>email</th>\n",
       "      <th>store_id</th>\n",
       "      <th>active</th>\n",
       "      <th>username</th>\n",
       "      <th>password</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mike</td>\n",
       "      <td>Hillyer</td>\n",
       "      <td>3</td>\n",
       "      <td>Mike.Hillyer@sakilastaff.com</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Mike</td>\n",
       "      <td>8cb2237d0679ca88db6464eac60da96345513964</td>\n",
       "      <td>2020-05-16 15:13:11.793280+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jon</td>\n",
       "      <td>Stephens</td>\n",
       "      <td>4</td>\n",
       "      <td>Jon.Stephens@sakilastaff.com</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Jon</td>\n",
       "      <td>8cb2237d0679ca88db6464eac60da96345513964</td>\n",
       "      <td>2020-05-16 15:13:11.793280+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   staff_id first_name last_name  address_id                         email  \\\n",
       "0         1       Mike   Hillyer           3  Mike.Hillyer@sakilastaff.com   \n",
       "1         2        Jon  Stephens           4  Jon.Stephens@sakilastaff.com   \n",
       "\n",
       "   store_id  active username                                  password  \\\n",
       "0         1    True     Mike  8cb2237d0679ca88db6464eac60da96345513964   \n",
       "1         2    True      Jon  8cb2237d0679ca88db6464eac60da96345513964   \n",
       "\n",
       "                       last_update  \n",
       "0 2020-05-16 15:13:11.793280+00:00  \n",
       "1 2020-05-16 15:13:11.793280+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_staff.drop(columns=['picture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836e39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df_actor = spark.createDataFrame(pd_actor)\n",
    "df_address = spark.createDataFrame(pd_address)\n",
    "df_category = spark.createDataFrame(pd_category)\n",
    "df_city = spark.createDataFrame(pd_city)\n",
    "df_country = spark.createDataFrame(pd_country)\n",
    "df_customer = spark.createDataFrame(pd_custormer)\n",
    "\n",
    "film_schemas = StructType([\n",
    "    StructField('film_id', IntegerType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('description', StringType(), True),\n",
    "    StructField('release_year', IntegerType(), True),\n",
    "    StructField('language_id', IntegerType(), True),\n",
    "    StructField('original_language_id', IntegerType(), True),\n",
    "    StructField('rental_duration', IntegerType(), True),\n",
    "    StructField('rental_rate', DoubleType(), True),\n",
    "    StructField('length', IntegerType(), True),\n",
    "    StructField('replacement_cost', DoubleType(), True),\n",
    "    StructField('rating', StringType(), True),\n",
    "    StructField('last_update', TimestampType(), True),\n",
    "    StructField('special_features', StringType(), True),\n",
    "    StructField('fulltext', StringType(), True),\n",
    "])\n",
    "\n",
    "df_film = spark.createDataFrame(pd_film, schema=film_schemas)\n",
    "df_film_actor = spark.createDataFrame(pd_film_actor)\n",
    "df_film_category = spark.createDataFrame(pd_film_category)\n",
    "df_invenotry = spark.createDataFrame(pd_invenotory)\n",
    "df_language = spark.createDataFrame(pd_language)\n",
    "df_payment = spark.createDataFrame(pd_payment)\n",
    "df_rental = spark.createDataFrame(pd_rental)\n",
    "\n",
    "staff_schemas = StructType([\n",
    "    StructField('staff_id', IntegerType(), True),\n",
    "    StructField('first_name', StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('address_id', IntegerType(), True),\n",
    "    StructField('email', StringType(), True),\n",
    "    StructField('store_id', IntegerType(), True),\n",
    "    StructField('active', IntegerType(), True),\n",
    "    StructField('username', StringType(), True),\n",
    "    StructField('password', StringType(), True),\n",
    "    StructField('last_update', TimestampType(), True),\n",
    "])\n",
    "\n",
    "df_staff = spark.createDataFrame(pd_staff.drop(columns=['picture']))\n",
    "df_store = spark.createDataFrame(pd_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eaa4dd",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f61caa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|       name|count_of_films|\n",
      "+-----------+--------------+\n",
      "|      Music|            51|\n",
      "|     Horror|            56|\n",
      "|   Classics|            57|\n",
      "|     Travel|            57|\n",
      "|     Comedy|            58|\n",
      "|   Children|            60|\n",
      "|     Sci-Fi|            61|\n",
      "|      Games|            61|\n",
      "|      Drama|            62|\n",
      "|        New|            63|\n",
      "|     Action|            64|\n",
      "|  Animation|            66|\n",
      "|Documentary|            68|\n",
      "|     Family|            69|\n",
      "|    Foreign|            73|\n",
      "|     Sports|            74|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "sum_film_in_category = df_film.join(df_film_category, df_film.film_id == df_film_category.film_id, 'inner')\\\n",
    "    .select(df_film.film_id, df_film_category.category_id).alias('sum_film_in_category').groupBy(col('category_id'))\\\n",
    "    .agg(\n",
    "        f.count(col('sum_film_in_category.film_id')).alias('count_of_films')\n",
    "    )\n",
    "\n",
    "sum_film_in_category = sum_film_in_category\\\n",
    "    .join(df_category, df_category.category_id == sum_film_in_category.category_id, 'inner')\\\n",
    "    .select(df_category.name, sum_film_in_category.count_of_films).sort(col('count_of_films').asc())\n",
    "sum_film_in_category.write.format('com.databricks.spark.csv').save('task_1_pyspark')\n",
    "sum_film_in_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4f89e",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f0f99d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+-----------------+\n",
      "|actor_id|first_name|  last_name| sum_of_diff_time|\n",
      "+--------+----------+-----------+-----------------+\n",
      "|      90|      SEAN|    GUINESS|69961.83333333342|\n",
      "|      65|    ANGELA|     HUDSON|70746.66666666672|\n",
      "|      37|       VAL|     BOLGER| 70863.2166666666|\n",
      "|     150|     JAYNE|      NOLTE|71786.23333333331|\n",
      "|      23|    SANDRA|     KILMER|72139.05000000008|\n",
      "|     102|    WALTER|       TORN|76949.60000000005|\n",
      "|     144|    ANGELA|WITHERSPOON|77669.46666666662|\n",
      "|     181|   MATTHEW|     CARREY|78829.46666666669|\n",
      "|     198|      MARY|     KEITEL|80679.18333333336|\n",
      "|     107|      GINA|  DEGENERES|         91217.05|\n",
      "+--------+----------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_table_from_actor = df_rental.join(df_invenotry, df_rental.inventory_id == df_invenotry.inventory_id, 'inner')\\\n",
    "    .join(df_film_actor, df_film_actor.film_id == df_invenotry.film_id, 'inner')\\\n",
    "    .select(df_film_actor.actor_id, df_rental.return_date, df_rental.rental_date)\n",
    "\n",
    "rental_table_from_actor = rental_table_from_actor\\\n",
    "    .withColumn('diff_time_in_hour', (col('return_date').cast('long') - col('rental_date').cast('long'))/3600 )\\\n",
    "    .select('actor_id', 'diff_time_in_hour')\n",
    "\n",
    "rental_table_from_actor = rental_table_from_actor.alias('rental_table_from_actor').groupBy(col('actor_id')).agg(\n",
    "        f.sum(col('diff_time_in_hour')).alias('sum_of_diff_time')\n",
    "    ).sort(col('sum_of_diff_time').desc()).limit(10)\\\n",
    "    .join(df_actor, df_actor.actor_id == rental_table_from_actor.actor_id, 'inner')\\\n",
    "    .select(df_actor.actor_id, df_actor.first_name, df_actor.last_name, col('sum_of_diff_time'))\\\n",
    "    .sort(col('sum_of_diff_time').asc())\n",
    "rental_table_from_actor.write.format('com.databricks.spark.csv').save('task_2_pyspark')\n",
    "rental_table_from_actor.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bceea",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900b61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  name|       sum_of_amount|\n",
      "+------+--------------------+\n",
      "|Sports|1.1599577850500427E7|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_table = df_payment.join(df_store, df_store.manager_staff_id == df_payment.staff_id, 'inner')\\\n",
    "    .join(df_invenotry, df_invenotry.store_id == df_store.store_id, 'inner')\\\n",
    "    .join(df_film_category, df_film_category.film_id == df_invenotry.film_id)\\\n",
    "    .select(df_film_category.category_id, df_payment.amount).groupBy(col('category_id')).agg(\n",
    "        f.sum(col('amount')).alias('sum_of_amount')\n",
    "    )\n",
    "result_table = result_table.sort(col('sum_of_amount').desc()).limit(1)\\\n",
    "    .join(df_category, df_category.category_id == result_table.category_id, 'inner')\\\n",
    "    .select(col('name'), col('sum_of_amount'))\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_3_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da6c5d",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57e036fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|film_id|               title|\n",
      "+-------+--------------------+\n",
      "|    198|    CRYSTAL BREAKING|\n",
      "|    325|       FLOATS GARDEN|\n",
      "|    144| CHINATOWN GLADIATOR|\n",
      "|    909|    TREASURE COMMAND|\n",
      "|    386|           GUMP DATE|\n",
      "|    642|      ORDER BETRAYED|\n",
      "|    713|       RAINBOW SHOCK|\n",
      "|     87|   BOONDOCK BALLROOM|\n",
      "|    221|DELIVERANCE MULHO...|\n",
      "|    712|   RAIDERS ANTITRUST|\n",
      "|     41|ARSENIC INDEPENDENCE|\n",
      "|     33|         APOLLO TEEN|\n",
      "|    671|     PERDITION FARGO|\n",
      "|    742|       ROOF CHAMPION|\n",
      "|    404|       HATE HANDICAP|\n",
      "|    419|         HOCUS FRIDA|\n",
      "|    874|        TADPOLE PARK|\n",
      "|    128|       CATCH AMISTAD|\n",
      "|    497|    KILL BROTHERHOOD|\n",
      "|    332|FRANKENSTEIN STRA...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_table = df_film.join(df_invenotry, df_invenotry.film_id == df_film.film_id, 'left')\\\n",
    "    .select(df_invenotry.inventory_id, df_film.film_id, df_film.title).filter(col('inventory_id').isNull())\\\n",
    "    .drop(col('inventory_id'))\n",
    "result_table.show()\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_4_pyspark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370da2d6",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3cde58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+-----+\n",
      "|actor_id|first_name|last_name|count|\n",
      "+--------+----------+---------+-----+\n",
      "|      17|     HELEN|   VOIGHT|    7|\n",
      "|     127|     KEVIN|  GARLAND|    5|\n",
      "|      80|     RALPH|     CRUZ|    5|\n",
      "|      66|      MARY|    TANDY|    5|\n",
      "|     140|    WHOOPI|     HURT|    5|\n",
      "+--------+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "result_table = df_film.join(df_film_category, df_film_category.film_id == df_film.film_id, 'inner')\\\n",
    "    .join(df_film_actor, df_film_actor.film_id == df_film.film_id)\\\n",
    "    .select(df_film_actor.film_id, df_film_actor.actor_id, df_film_category.category_id)\n",
    "\n",
    "result_table = result_table.join(df_category, df_category.category_id == result_table.category_id)\\\n",
    "    .filter(col('name') == 'Children').select(col('film_id'), col('actor_id'))\\\n",
    "    .groupBy(col('actor_id')).agg(\n",
    "        f.count(col('film_id')).alias('count')\n",
    "    )\n",
    "\n",
    "w = Window.orderBy(f.desc(\"count\"))\n",
    "result_table = result_table.select(col('actor_id'), col('count'), rank().over(w).alias('rank'))\\\n",
    "    .filter(col('rank') < 4).drop('rank').join(df_actor, df_actor.actor_id == result_table.actor_id, 'inner')\\\n",
    "    .select(df_actor.actor_id, df_actor.first_name, df_actor.last_name, col('count'))\n",
    "\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_5_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dcdae",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f32e60fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+----------------+\n",
      "|city_id|             city|active_count|non_active_count|\n",
      "+-------+-----------------+------------+----------------+\n",
      "|     26|          Anpolis|           1|               0|\n",
      "|     29|        Apeldoorn|           1|               0|\n",
      "|    474|      Shimonoseki|           1|               0|\n",
      "|     65|         Bellevue|           1|               0|\n",
      "|    191|         Gulbarga|           1|               0|\n",
      "|    418|  Purnea (Purnia)|           1|               0|\n",
      "|    541|           Torren|           1|               0|\n",
      "|    558|Usolje-Sibirskoje|           1|               0|\n",
      "|    222|            Inegl|           1|               0|\n",
      "|    270|   Kirovo-Tepetsk|           1|               0|\n",
      "|    293|         Laohekou|           1|               0|\n",
      "|    243|          Jodhpur|           1|               0|\n",
      "|    278|          Korolev|           1|               0|\n",
      "|    367|       Niznekamsk|           1|               0|\n",
      "|    442|      Saint-Denis|           1|               0|\n",
      "|     19|          Allende|           1|               0|\n",
      "|     54|           Banjul|           1|               0|\n",
      "|    296|         Lausanne|           1|               0|\n",
      "|    277|            Korla|           1|               0|\n",
      "|    287|           Kuwana|           1|               0|\n",
      "+-------+-----------------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_with_active_and_nonactive = df_customer\\\n",
    "    .join(df_address, df_address.address_id == df_customer.address_id, 'inner')\\\n",
    "    .join(df_city, df_city.city_id == df_address.city_id)\\\n",
    "    .select(df_customer.customer_id, df_customer.activebool, df_city.city_id)\n",
    "\n",
    "active_customer = table_with_active_and_nonactive.filter(col('activebool') == 1)\n",
    "non_active_customer = table_with_active_and_nonactive.filter(col('activebool') == 0)\n",
    "\n",
    "active_customer = active_customer.groupBy(col('city_id').alias('act_city_id')).agg(\n",
    "        f.count(col('customer_id')).alias('active_count')\n",
    "    )\n",
    "\n",
    "non_active_customer = non_active_customer.groupBy(col('city_id').alias('non_act_city_id')).agg(\n",
    "        f.count(col('customer_id')).alias('non_active_count')\n",
    "    )\n",
    "\n",
    "result_table = df_city.join(active_customer, active_customer.act_city_id == df_city.city_id, 'left')\\\n",
    "    .drop(col('act_city_id')).join(non_active_customer, non_active_customer.non_act_city_id == df_city.city_id, 'left')\\\n",
    "    .drop(col('non_act_city_id')).select(col('city_id'), col('city'), col('active_count'), col('non_active_count'))\\\n",
    "    .na.fill(0, subset=['active_count', 'non_active_count']).sort(col('non_active_count').asc())\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_6_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ab77c",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d91e499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rental_id',\n",
       " 'rental_date',\n",
       " 'inventory_id',\n",
       " 'customer_id',\n",
       " 'return_date',\n",
       " 'staff_id',\n",
       " 'last_update']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rental.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a39aff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rental_id: bigint, inventory_id: bigint, customer_id: bigint, staff_id: bigint, diff_time_of_hour: double]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rental_with_diff_time = df_rental\\\n",
    "    .withColumn('diff_time_of_hour', (col('return_date').cast('long') - col('rental_date').cast('long'))/3600)\\\n",
    "    .select(col('rental_id'), col('inventory_id'), col('customer_id'), col('staff_id'), col('diff_time_of_hour'))\n",
    "df_rental_with_diff_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cb2c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------------+\n",
      "|                city|       name|sum_of_diff_time_of_hour|\n",
      "+--------------------+-----------+------------------------+\n",
      "|          Avellaneda|     Action|      1109.8666666666666|\n",
      "|           Lapu-Lapu|     Comedy|       898.7499999999999|\n",
      "|         Saint-Denis|     Sci-Fi|       814.6166666666667|\n",
      "|Donostia-San Seba...|     Sports|       800.3833333333333|\n",
      "|                Aden|        New|       795.2333333333333|\n",
      "|           Apeldoorn|     Sports|       776.3666666666667|\n",
      "| Aparecida de Goinia|     Travel|       754.3499999999999|\n",
      "|              Aurora|Documentary|       707.9166666666666|\n",
      "| Kamjanets-Podilskyi|   Children|       705.9666666666667|\n",
      "|              Ashdod|     Sci-Fi|       699.7333333333333|\n",
      "|    Shubra al-Khayma|     Sci-Fi|       695.8333333333333|\n",
      "|              Atinsk|     Travel|       695.7333333333333|\n",
      "|             Allende|     Travel|       685.8166666666666|\n",
      "|             Arecibo|        New|       684.4833333333333|\n",
      "|        Effon-Alaiye|      Drama|       680.1666666666666|\n",
      "|            Akishima|   Children|                  652.65|\n",
      "|           al-Manama|     Action|                   652.0|\n",
      "|            Ashgabat|     Family|       647.3833333333334|\n",
      "|            al-Qatif|     Sci-Fi|       638.6833333333333|\n",
      "|             Athenai|     Action|       621.7333333333332|\n",
      "+--------------------+-----------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_table = df_rental_with_diff_time\\\n",
    "    .join(df_invenotry, df_invenotry.inventory_id == df_rental_with_diff_time.inventory_id, 'inner')\\\n",
    "    .join(df_film_category, df_film_category.film_id == df_invenotry.film_id, 'inner')\\\n",
    "    .join(df_customer, df_customer.customer_id == df_rental_with_diff_time.customer_id, 'inner')\\\n",
    "    .join(df_address, df_address.address_id == df_customer.address_id, 'inner')\\\n",
    "    .join(df_city, df_city.city_id == df_address.city_id, 'inner')\\\n",
    "    .select(df_film_category.category_id, df_city.city_id, df_rental_with_diff_time.diff_time_of_hour)\n",
    "\n",
    "w = Window.partitionBy('city_id').orderBy(f.desc('sum_of_diff_time_of_hour'))\n",
    "result_table = result_table.groupBy(col('city_id'), col('category_id')).agg(\n",
    "        f.sum(col('diff_time_of_hour')).alias('sum_of_diff_time_of_hour')\n",
    "    ).na.fill(0, subset=['sum_of_diff_time_of_hour']).sort(col('city_id'), col('category_id'))\\\n",
    "    .withColumn('rank', rank().over(w)).filter(col('rank') == 1).drop(col('rank'))\n",
    "\n",
    "result_table = result_table.join(df_city, df_city.city_id == result_table.city_id, 'inner')\\\n",
    "    .select(col('city'), col('category_id'), col('sum_of_diff_time_of_hour'))\\\n",
    "    .join(df_category, df_category.category_id == result_table.category_id)\\\n",
    "    .select(col('city'), df_category.name, col('sum_of_diff_time_of_hour'))\\\n",
    "    .withColumn('lower_city', f.lower(col('city')))\\\n",
    "    .filter((col('lower_city').startswith('a') | col('lower_city').contains('-')))\\\n",
    "    .drop(col('lower_city')).sort(col('sum_of_diff_time_of_hour').desc())\n",
    "\n",
    "result_table.repartition(3).write.format('com.databricks.spark.csv').save('task_7_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6fb89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0440f771c91ab0cddb93644871454d02a51ae0f94f4adabb2f4fc725ed664ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
