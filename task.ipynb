{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2878b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"flask_app\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f426a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ce2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine(\"postgresql+psycopg2://root:root@localhost/test_db?client_encoding=utf8\")\n",
    "pd_actor = pd.read_sql('select * from public.actor', engine)\n",
    "pd_address = pd.read_sql('select * from public.address', engine)\n",
    "pd_category = pd.read_sql('select * from public.category', engine)\n",
    "pd_city = pd.read_sql('select * from public.city', engine)\n",
    "pd_country = pd.read_sql('select * from public.country', engine)\n",
    "pd_custormer = pd.read_sql('select * from public.customer', engine)\n",
    "pd_film = pd.read_sql('select * from public.film', engine)\n",
    "pd_film_actor = pd.read_sql('select * from public.film_actor', engine)\n",
    "pd_film_category = pd.read_sql('select * from public.film_category', engine)\n",
    "pd_invenotory = pd.read_sql('select * from public.inventory', engine)\n",
    "pd_language = pd.read_sql('select * from public.language', engine)\n",
    "pd_payment = pd.read_sql('select * from public.payment', engine)\n",
    "pd_rental = pd.read_sql('select * from public.rental', engine)\n",
    "pd_staff = pd.read_sql('select * from public.staff', engine)\n",
    "pd_store = pd.read_sql('select * from public.store', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3e75d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>staff_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>address_id</th>\n",
       "      <th>email</th>\n",
       "      <th>store_id</th>\n",
       "      <th>active</th>\n",
       "      <th>username</th>\n",
       "      <th>password</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mike</td>\n",
       "      <td>Hillyer</td>\n",
       "      <td>3</td>\n",
       "      <td>Mike.Hillyer@sakilastaff.com</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Mike</td>\n",
       "      <td>8cb2237d0679ca88db6464eac60da96345513964</td>\n",
       "      <td>2020-05-16 15:13:11.793280+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jon</td>\n",
       "      <td>Stephens</td>\n",
       "      <td>4</td>\n",
       "      <td>Jon.Stephens@sakilastaff.com</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Jon</td>\n",
       "      <td>8cb2237d0679ca88db6464eac60da96345513964</td>\n",
       "      <td>2020-05-16 15:13:11.793280+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   staff_id first_name last_name  address_id                         email  \\\n",
       "0         1       Mike   Hillyer           3  Mike.Hillyer@sakilastaff.com   \n",
       "1         2        Jon  Stephens           4  Jon.Stephens@sakilastaff.com   \n",
       "\n",
       "   store_id  active username                                  password  \\\n",
       "0         1    True     Mike  8cb2237d0679ca88db6464eac60da96345513964   \n",
       "1         2    True      Jon  8cb2237d0679ca88db6464eac60da96345513964   \n",
       "\n",
       "                       last_update  \n",
       "0 2020-05-16 15:13:11.793280+00:00  \n",
       "1 2020-05-16 15:13:11.793280+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_staff.drop(columns=['picture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836e39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df_actor = spark.createDataFrame(pd_actor)\n",
    "df_address = spark.createDataFrame(pd_address)\n",
    "df_category = spark.createDataFrame(pd_category)\n",
    "df_city = spark.createDataFrame(pd_city)\n",
    "df_country = spark.createDataFrame(pd_country)\n",
    "df_customer = spark.createDataFrame(pd_custormer)\n",
    "\n",
    "film_schemas = StructType([\n",
    "    StructField('film_id', IntegerType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('description', StringType(), True),\n",
    "    StructField('release_year', IntegerType(), True),\n",
    "    StructField('language_id', IntegerType(), True),\n",
    "    StructField('original_language_id', IntegerType(), True),\n",
    "    StructField('rental_duration', IntegerType(), True),\n",
    "    StructField('rental_rate', DoubleType(), True),\n",
    "    StructField('length', IntegerType(), True),\n",
    "    StructField('replacement_cost', DoubleType(), True),\n",
    "    StructField('rating', StringType(), True),\n",
    "    StructField('last_update', TimestampType(), True),\n",
    "    StructField('special_features', StringType(), True),\n",
    "    StructField('fulltext', StringType(), True),\n",
    "])\n",
    "\n",
    "df_film = spark.createDataFrame(pd_film, schema=film_schemas)\n",
    "df_film_actor = spark.createDataFrame(pd_film_actor)\n",
    "df_film_category = spark.createDataFrame(pd_film_category)\n",
    "df_invenotry = spark.createDataFrame(pd_invenotory)\n",
    "df_language = spark.createDataFrame(pd_language)\n",
    "df_payment = spark.createDataFrame(pd_payment)\n",
    "df_rental = spark.createDataFrame(pd_rental)\n",
    "\n",
    "staff_schemas = StructType([\n",
    "    StructField('staff_id', IntegerType(), True),\n",
    "    StructField('first_name', StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('address_id', IntegerType(), True),\n",
    "    StructField('email', StringType(), True),\n",
    "    StructField('store_id', IntegerType(), True),\n",
    "    StructField('active', IntegerType(), True),\n",
    "    StructField('username', StringType(), True),\n",
    "    StructField('password', StringType(), True),\n",
    "    StructField('last_update', TimestampType(), True),\n",
    "])\n",
    "\n",
    "df_staff = spark.createDataFrame(pd_staff.drop(columns=['picture']))\n",
    "df_store = spark.createDataFrame(pd_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eaa4dd",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f61caa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|       name|count_of_films|\n",
      "+-----------+--------------+\n",
      "|      Music|            51|\n",
      "|     Horror|            56|\n",
      "|   Classics|            57|\n",
      "|     Travel|            57|\n",
      "|     Comedy|            58|\n",
      "|   Children|            60|\n",
      "|     Sci-Fi|            61|\n",
      "|      Games|            61|\n",
      "|      Drama|            62|\n",
      "|        New|            63|\n",
      "|     Action|            64|\n",
      "|  Animation|            66|\n",
      "|Documentary|            68|\n",
      "|     Family|            69|\n",
      "|    Foreign|            73|\n",
      "|     Sports|            74|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "sum_film_in_category = df_film.join(df_film_category, df_film.film_id == df_film_category.film_id, 'inner')\\\n",
    "    .select(df_film.film_id, df_film_category.category_id).alias('sum_film_in_category').groupBy(col('category_id'))\\\n",
    "    .agg(\n",
    "        f.count(col('sum_film_in_category.film_id')).alias('count_of_films')\n",
    "    )\n",
    "\n",
    "sum_film_in_category = sum_film_in_category\\\n",
    "    .join(df_category, df_category.category_id == sum_film_in_category.category_id, 'inner')\\\n",
    "    .select(df_category.name, sum_film_in_category.count_of_films).sort(col('count_of_films').asc())\n",
    "sum_film_in_category.write.format('com.databricks.spark.csv').save('task_1_pyspark')\n",
    "sum_film_in_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4f89e",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f0f99d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+-----------------+\n",
      "|actor_id|first_name|  last_name| sum_of_diff_time|\n",
      "+--------+----------+-----------+-----------------+\n",
      "|      90|      SEAN|    GUINESS|69961.83333333342|\n",
      "|      65|    ANGELA|     HUDSON|70746.66666666672|\n",
      "|      37|       VAL|     BOLGER| 70863.2166666666|\n",
      "|     150|     JAYNE|      NOLTE|71786.23333333331|\n",
      "|      23|    SANDRA|     KILMER|72139.05000000008|\n",
      "|     102|    WALTER|       TORN|76949.60000000005|\n",
      "|     144|    ANGELA|WITHERSPOON|77669.46666666662|\n",
      "|     181|   MATTHEW|     CARREY|78829.46666666669|\n",
      "|     198|      MARY|     KEITEL|80679.18333333336|\n",
      "|     107|      GINA|  DEGENERES|         91217.05|\n",
      "+--------+----------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_table_from_actor = df_rental.join(df_invenotry, df_rental.inventory_id == df_invenotry.inventory_id, 'inner')\\\n",
    "    .join(df_film_actor, df_film_actor.film_id == df_invenotry.film_id, 'inner')\\\n",
    "    .select(df_film_actor.actor_id, df_rental.return_date, df_rental.rental_date)\n",
    "\n",
    "rental_table_from_actor = rental_table_from_actor\\\n",
    "    .withColumn('diff_time_in_hour', (col('return_date').cast('long') - col('rental_date').cast('long'))/3600 )\\\n",
    "    .select('actor_id', 'diff_time_in_hour')\n",
    "\n",
    "rental_table_from_actor = rental_table_from_actor.alias('rental_table_from_actor').groupBy(col('actor_id')).agg(\n",
    "        f.sum(col('diff_time_in_hour')).alias('sum_of_diff_time')\n",
    "    ).sort(col('sum_of_diff_time').desc()).limit(10)\\\n",
    "    .join(df_actor, df_actor.actor_id == rental_table_from_actor.actor_id, 'inner')\\\n",
    "    .select(df_actor.actor_id, df_actor.first_name, df_actor.last_name, col('sum_of_diff_time'))\\\n",
    "    .sort(col('sum_of_diff_time').asc())\n",
    "rental_table_from_actor.write.format('com.databricks.spark.csv').save('task_2_pyspark')\n",
    "rental_table_from_actor.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bceea",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900b61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  name|       sum_of_amount|\n",
      "+------+--------------------+\n",
      "|Sports|1.1599577850500427E7|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_table = df_payment.join(df_store, df_store.manager_staff_id == df_payment.staff_id, 'inner')\\\n",
    "    .join(df_invenotry, df_invenotry.store_id == df_store.store_id, 'inner')\\\n",
    "    .join(df_film_category, df_film_category.film_id == df_invenotry.film_id)\\\n",
    "    .select(df_film_category.category_id, df_payment.amount).groupBy(col('category_id')).agg(\n",
    "        f.sum(col('amount')).alias('sum_of_amount')\n",
    "    )\n",
    "result_table = result_table.sort(col('sum_of_amount').desc()).limit(1)\\\n",
    "    .join(df_category, df_category.category_id == result_table.category_id, 'inner')\\\n",
    "    .select(col('name'), col('sum_of_amount'))\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_3_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da6c5d",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57e036fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|film_id|               title|\n",
      "+-------+--------------------+\n",
      "|    198|    CRYSTAL BREAKING|\n",
      "|    325|       FLOATS GARDEN|\n",
      "|    144| CHINATOWN GLADIATOR|\n",
      "|    909|    TREASURE COMMAND|\n",
      "|    386|           GUMP DATE|\n",
      "|    642|      ORDER BETRAYED|\n",
      "|    713|       RAINBOW SHOCK|\n",
      "|     87|   BOONDOCK BALLROOM|\n",
      "|    221|DELIVERANCE MULHO...|\n",
      "|    712|   RAIDERS ANTITRUST|\n",
      "|     41|ARSENIC INDEPENDENCE|\n",
      "|     33|         APOLLO TEEN|\n",
      "|    671|     PERDITION FARGO|\n",
      "|    742|       ROOF CHAMPION|\n",
      "|    404|       HATE HANDICAP|\n",
      "|    419|         HOCUS FRIDA|\n",
      "|    874|        TADPOLE PARK|\n",
      "|    128|       CATCH AMISTAD|\n",
      "|    497|    KILL BROTHERHOOD|\n",
      "|    332|FRANKENSTEIN STRA...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_table = df_film.join(df_invenotry, df_invenotry.film_id == df_film.film_id, 'left')\\\n",
    "    .select(df_invenotry.inventory_id, df_film.film_id, df_film.title).filter(col('inventory_id').isNull())\\\n",
    "    .drop(col('inventory_id'))\n",
    "result_table.show()\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_4_pyspark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370da2d6",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3cde58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+-----+\n",
      "|actor_id|first_name|last_name|count|\n",
      "+--------+----------+---------+-----+\n",
      "|      17|     HELEN|   VOIGHT|    7|\n",
      "|     127|     KEVIN|  GARLAND|    5|\n",
      "|      80|     RALPH|     CRUZ|    5|\n",
      "|      66|      MARY|    TANDY|    5|\n",
      "|     140|    WHOOPI|     HURT|    5|\n",
      "+--------+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "result_table = df_film.join(df_film_category, df_film_category.film_id == df_film.film_id, 'inner')\\\n",
    "    .join(df_film_actor, df_film_actor.film_id == df_film.film_id)\\\n",
    "    .select(df_film_actor.film_id, df_film_actor.actor_id, df_film_category.category_id)\n",
    "\n",
    "result_table = result_table.join(df_category, df_category.category_id == result_table.category_id)\\\n",
    "    .filter(col('name') == 'Children').select(col('film_id'), col('actor_id'))\\\n",
    "    .groupBy(col('actor_id')).agg(\n",
    "        f.count(col('film_id')).alias('count')\n",
    "    )\n",
    "\n",
    "w = Window.orderBy(f.desc(\"count\"))\n",
    "result_table = result_table.select(col('actor_id'), col('count'), rank().over(w).alias('rank'))\\\n",
    "    .filter(col('rank') < 4).drop('rank').join(df_actor, df_actor.actor_id == result_table.actor_id, 'inner')\\\n",
    "    .select(df_actor.actor_id, df_actor.first_name, df_actor.last_name, col('count'))\n",
    "\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_5_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dcdae",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f32e60fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+----------------+\n",
      "|city_id|             city|active_count|non_active_count|\n",
      "+-------+-----------------+------------+----------------+\n",
      "|     26|          Anpolis|           1|               0|\n",
      "|     29|        Apeldoorn|           1|               0|\n",
      "|    474|      Shimonoseki|           1|               0|\n",
      "|     65|         Bellevue|           1|               0|\n",
      "|    191|         Gulbarga|           1|               0|\n",
      "|    418|  Purnea (Purnia)|           1|               0|\n",
      "|    541|           Torren|           1|               0|\n",
      "|    558|Usolje-Sibirskoje|           1|               0|\n",
      "|    222|            Inegl|           1|               0|\n",
      "|    270|   Kirovo-Tepetsk|           1|               0|\n",
      "|    293|         Laohekou|           1|               0|\n",
      "|    243|          Jodhpur|           1|               0|\n",
      "|    278|          Korolev|           1|               0|\n",
      "|    367|       Niznekamsk|           1|               0|\n",
      "|    442|      Saint-Denis|           1|               0|\n",
      "|     19|          Allende|           1|               0|\n",
      "|     54|           Banjul|           1|               0|\n",
      "|    296|         Lausanne|           1|               0|\n",
      "|    277|            Korla|           1|               0|\n",
      "|    287|           Kuwana|           1|               0|\n",
      "+-------+-----------------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_with_active_and_nonactive = df_customer\\\n",
    "    .join(df_address, df_address.address_id == df_customer.address_id, 'inner')\\\n",
    "    .join(df_city, df_city.city_id == df_address.city_id)\\\n",
    "    .select(df_customer.customer_id, df_customer.activebool, df_city.city_id)\n",
    "\n",
    "active_customer = table_with_active_and_nonactive.filter(col('activebool') == 1)\n",
    "non_active_customer = table_with_active_and_nonactive.filter(col('activebool') == 0)\n",
    "\n",
    "active_customer = active_customer.groupBy(col('city_id').alias('act_city_id')).agg(\n",
    "        f.count(col('customer_id')).alias('active_count')\n",
    "    )\n",
    "\n",
    "non_active_customer = non_active_customer.groupBy(col('city_id').alias('non_act_city_id')).agg(\n",
    "        f.count(col('customer_id')).alias('non_active_count')\n",
    "    )\n",
    "\n",
    "result_table = df_city.join(active_customer, active_customer.act_city_id == df_city.city_id, 'left')\\\n",
    "    .drop(col('act_city_id')).join(non_active_customer, non_active_customer.non_act_city_id == df_city.city_id, 'left')\\\n",
    "    .drop(col('non_act_city_id')).select(col('city_id'), col('city'), col('active_count'), col('non_active_count'))\\\n",
    "    .na.fill(0, subset=['active_count', 'non_active_count']).sort(col('non_active_count').asc())\n",
    "result_table.write.format('com.databricks.spark.csv').save('task_6_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ab77c",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d91e499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rental_id',\n",
       " 'rental_date',\n",
       " 'inventory_id',\n",
       " 'customer_id',\n",
       " 'return_date',\n",
       " 'staff_id',\n",
       " 'last_update']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rental.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a39aff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rental_id: bigint, inventory_id: bigint, customer_id: bigint, staff_id: bigint, diff_time_of_hour: double]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rental_with_diff_time = df_rental\\\n",
    "    .withColumn('diff_time_of_hour', (col('return_date').cast('long') - col('rental_date').cast('long'))/3600)\\\n",
    "    .select(col('rental_id'), col('inventory_id'), col('customer_id'), col('staff_id'), col('diff_time_of_hour'))\n",
    "df_rental_with_diff_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cb2c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------------+\n",
      "|                city|       name|sum_of_diff_time_of_hour|\n",
      "+--------------------+-----------+------------------------+\n",
      "|          Avellaneda|     Action|      1109.8666666666666|\n",
      "|           Lapu-Lapu|     Comedy|       898.7499999999999|\n",
      "|         Saint-Denis|     Sci-Fi|       814.6166666666667|\n",
      "|Donostia-San Seba...|     Sports|       800.3833333333333|\n",
      "|                Aden|        New|       795.2333333333333|\n",
      "|           Apeldoorn|     Sports|       776.3666666666667|\n",
      "| Aparecida de Goinia|     Travel|       754.3499999999999|\n",
      "|              Aurora|Documentary|       707.9166666666666|\n",
      "| Kamjanets-Podilskyi|   Children|       705.9666666666667|\n",
      "|              Ashdod|     Sci-Fi|       699.7333333333333|\n",
      "|    Shubra al-Khayma|     Sci-Fi|       695.8333333333333|\n",
      "|              Atinsk|     Travel|       695.7333333333333|\n",
      "|             Allende|     Travel|       685.8166666666666|\n",
      "|             Arecibo|        New|       684.4833333333333|\n",
      "|        Effon-Alaiye|      Drama|       680.1666666666666|\n",
      "|            Akishima|   Children|                  652.65|\n",
      "|           al-Manama|     Action|                   652.0|\n",
      "|            Ashgabat|     Family|       647.3833333333334|\n",
      "|            al-Qatif|     Sci-Fi|       638.6833333333333|\n",
      "|             Athenai|     Action|       621.7333333333332|\n",
      "+--------------------+-----------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_table = df_rental_with_diff_time\\\n",
    "    .join(df_invenotry, df_invenotry.inventory_id == df_rental_with_diff_time.inventory_id, 'inner')\\\n",
    "    .join(df_film_category, df_film_category.film_id == df_invenotry.film_id, 'inner')\\\n",
    "    .join(df_customer, df_customer.customer_id == df_rental_with_diff_time.customer_id, 'inner')\\\n",
    "    .join(df_address, df_address.address_id == df_customer.address_id, 'inner')\\\n",
    "    .join(df_city, df_city.city_id == df_address.city_id, 'inner')\\\n",
    "    .select(df_film_category.category_id, df_city.city_id, df_rental_with_diff_time.diff_time_of_hour)\n",
    "\n",
    "w = Window.partitionBy('city_id').orderBy(f.desc('sum_of_diff_time_of_hour'))\n",
    "result_table = result_table.groupBy(col('city_id'), col('category_id')).agg(\n",
    "        f.sum(col('diff_time_of_hour')).alias('sum_of_diff_time_of_hour')\n",
    "    ).na.fill(0, subset=['sum_of_diff_time_of_hour']).sort(col('city_id'), col('category_id'))\\\n",
    "    .withColumn('rank', rank().over(w)).filter(col('rank') == 1).drop(col('rank'))\n",
    "\n",
    "result_table = result_table.join(df_city, df_city.city_id == result_table.city_id, 'inner')\\\n",
    "    .select(col('city'), col('category_id'), col('sum_of_diff_time_of_hour'))\\\n",
    "    .join(df_category, df_category.category_id == result_table.category_id)\\\n",
    "    .select(col('city'), df_category.name, col('sum_of_diff_time_of_hour'))\\\n",
    "    .withColumn('lower_city', f.lower(col('city')))\\\n",
    "    .filter((col('lower_city').startswith('a') | col('lower_city').contains('-')))\\\n",
    "    .drop(col('lower_city')).sort(col('sum_of_diff_time_of_hour').desc())\n",
    "\n",
    "result_table.repartition(3).write.format('com.databricks.spark.csv').save('task_7_pyspark')\n",
    "result_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6fb89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
